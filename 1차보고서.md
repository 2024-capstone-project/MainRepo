<html>
  <body>
    <article id="1c318ebd-184f-808f-8230-ee6ea25da6a6" class="page sans">
      <div class="page-body">
        <h1 id="1c618ebd-184f-812a-a9e5-c8950fee6525" class="">Team-Info</h1>
        <table id="1c618ebd-184f-810a-a7fb-c5e5b359dcd3" class="simple-table">
          <tbody>
            <tr id="1c618ebd-184f-81ab-8d66-e1a277beea6e">
              <td id="v{KO" class="">(1) 과제명</td>
              <td id="DJ@Z" class="" style="width: 1248px">
                시선추적, 제스처인식, STT 기반 AI 발표 트레이너, SPitching
              </td>
            </tr>
            <tr id="1c618ebd-184f-8136-bf80-cfc2532abe8b">
              <td id="v{KO" class="">(2) 팀 번호 / 팀 이름</td>
              <td id="DJ@Z" class="" style="width: 1248px">24-대장정</td>
            </tr>
            <tr id="1c618ebd-184f-81a7-a409-d0d8d0aef730">
              <td id="v{KO" class="">(3) 팀 구성원</td>
              <td id="DJ@Z" class="" style="width: 1248px">
                이소정 (2122044): 리더, 백엔드, AI<br />장다예 (2171042): 팀원,
                백엔드, AI<br />한서정 (2217038) : 팀원, 프론트엔드<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-8125-9f67-ebe691ee4103">
              <td id="v{KO" class="">(4) 팀 지도교수</td>
              <td id="DJ@Z" class="" style="width: 1248px">심재형 교수님</td>
            </tr>
            <tr id="1c618ebd-184f-81c1-85c6-d886e1fcd17b">
              <td id="v{KO" class="">(5) 과제 분류</td>
              <td id="DJ@Z" class="" style="width: 1248px">산학과제</td>
            </tr>
            <tr id="1c618ebd-184f-81c2-b91d-fb3f6dbcf508">
              <td id="v{KO" class="">(6) 과제 키워드</td>
              <td id="DJ@Z" class="" style="width: 1248px">
                발표 습관 개선, 제스처 인식, 시선추적, STT, 생성형 AI
              </td>
            </tr>
            <tr id="1c618ebd-184f-8177-93a4-c5f3b6d6c9ef">
              <td id="v{KO" class="">(7) 과제 내용 요약</td>
              <td id="DJ@Z" class="" style="width: 1248px">
                드라마 스타트업의 수지처럼 능숙하고 유창한 발표를 할 수 있도록,
                시선추적 및 제스처인식, STT 기능을 통해 사용자의 발표습관을
                피드백해주고 생성형 AI를 통해 질의응답 대비 기능을 제공해주는
                발표 트레이너 서비스입니다.
              </td>
            </tr>
          </tbody>
        </table>
        <h1 id="1c618ebd-184f-8106-b8af-e0ee9e8e9175" class="">
          Project-Summary
        </h1>
        <table id="1c618ebd-184f-8182-aff8-c086d99c111b" class="simple-table">
          <tbody>
            <tr id="1c618ebd-184f-8127-a5e1-f5c514d030e0">
              <td id="J&lt;ip" class="">항목</td>
              <td id="WwZ~" class="" style="width: 1244px">내용</td>
            </tr>
            <tr id="1c618ebd-184f-819d-b07d-d00a13eaf9ab">
              <td id="J&lt;ip" class="">(1) 문제 정의</td>
              <td id="WwZ~" class="" style="width: 1244px">
                1. 시선처리 불안 : 떨림이는 발표할 때 청중을 보지 않고 대본만
                보고 달달 읽을 때도 있고 종종 허공만 바라보며 발표하기도
                합니다.<br /><br />2. 불필요한 제스처 : 떨림이는 당황할 때 손을
                입술에 갖다대거나 머리를 긁적이는 등 불필요하거나 과한 손동작을
                사용하기도 합니다.<br /><br />3. 불필요한 추임새 : 떨림이는
                긴장을 하면 말 더듬거나 어.., 음.., 그…와 같은 불필요한 추임새를
                넣습니다. <br /><br />4. 부정확한 발표 내용 : 떨림이는 발표할 때
                긴장하면 말의 순서가 다 꼬이고 중요한 내용을 빠뜨리기도 합니다.
                <br /><br />5. 질의응답에 대한 두려움 : 떨림이는 발표 이후
                질의응답에 대한 자신감 부족이 항상 큰 고민거리입니다.<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-8115-a219-f6ce6890a5a7">
              <td id="J&lt;ip" class="">(2) 기존연구와의 비교</td>
              <td id="WwZ~" class="" style="width: 1244px">
                Orai는 음성 피드백에만 집중해 비언어적 표현에 대한 분석이
                미흡하고, VirtualSpeech는 VR기기가 있어야만 연습이 가능하며,
                Microsoft의 Presentation Coach는 이전 발표 기록이 저장되지 않아
                발표력 향상률을 확인할 수 없다는 단점이 존재했습니다. 세 서비스
                모두 공통적으로 오직 영어로만 서비스가 제공된다는 치명적인
                한계도 존재했습니다. 따라서 저희 스피칭은 “한국어를 지원”하는
                “종합” 발표 트레이닝 서비스라는 점에서 차별점을 지닙니다.
              </td>
            </tr>
            <tr id="1c618ebd-184f-8133-be56-f353bed2e08e">
              <td id="J&lt;ip" class="">(3) 제안 내용</td>
              <td id="WwZ~" class="" style="width: 1244px">
                1. 시선 처리 피드백 → 발표 중 효과적인 아이컨택 연습을 도움<br /><br />2.
                다양한 제스처 탐지 → 발표력을 향상/저하 시키는 제스처를 인식하고
                습관을 개선해 나감<br /><br />3. 불필요한 추임새 피드백 →
                ‘어,음,그’ 의 빈도수를 제공함으로써 발표 유창성을 기르도록
                유도<br /><br />4. 대본 유사도 제공 → 기존 스크립트와 실제 발표
                연습 간의 내용 일치도 제공 <br /><br />5. 질의응답 대비 →
                발표연습 후 Q&amp;A 세션을 통해 청중과의 질의응답 연습<br /><br />*
                언어적 표현 2가지(발표유창성, 대본유사도), 비언어적 표현
                2가지(시선처리, 제스처)에 대한 종합 피드백 점수를 제공<br />*
                발표연습을 여러 번 반복함에 따라 이전 점수와 비교할 수 있도록
                발표력 향상률 그래프 및 레포트 제공<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-818d-90cd-c9067860b55d">
              <td id="J&lt;ip" class="">(4) 기대효과 및 의의</td>
              <td id="WwZ~" class="" style="width: 1244px">
                개인 맞춤형 발표습관 피드백을 받고 그에 맞는 발표 연습을
                반복함으로써 발표에 대한 불안감을 극복하게 해줍니다. 시간과
                공간의 제약 없이 발표 준비를 할 수 있습니다.
              </td>
            </tr>
            <tr id="1c618ebd-184f-8107-a0fd-df9d6a2410b0">
              <td id="J&lt;ip" class="">
                <strong>(5) 주요 기능 리스트</strong>
              </td>
              <td id="WwZ~" class="" style="width: 1244px">
                <strong>1. 시선 추적 분석 기능</strong><br />사용자의 고개
                방향을 분석하여 아이컨택 여부 및 시선 방향을 판단하고, 시선 점수
                및 피드백 영상을 생성합니다. 시선 추적 기능은 사용자의 시선을
                분석하여 발표의 집중도를 높이는데 도움을 줍니다. 발표 연습 중에
                사용자는 대본만 읽거나 허공을 바라보는 습관을 개선할 수
                있습니다.<br /><br /><br /><strong
                  >2. 제스처 인식 및 라벨링 기능</strong
                ><br />발표자의 긍정/부정 제스처를 실시간으로 감지하고, 빈도수와
                함께 라벨링된 영상 피드백을 제공합니다.<br /><br /><br /><strong
                  >3. 추임새 탐지 기능</strong
                ><br />STT 기반으로 &quot;어&quot;, &quot;음&quot;,
                &quot;그&quot;와 같은 추임새를 감지하고, 전체 발화 시간 대비
                사용 비율과 함께 피드백 합니다.<br /><br /><br /><strong
                  >4. 대본 유사도 분석 기능</strong
                ><br />발표가 끝난 후 사용자의 음성을 텍스트로 변환하고, 사전에
                입력한 대본과 비교하여 유사도 점수를 제공합니다. 연습 결과는
                정확도 보고서로 제공되며, 사용자는 이를 참고해 말하기 습관을
                개선하고 발표의 명확성과 자연스러움을 높일 수 있습니다.<br /><br /><br /><strong
                  >5. 맞춤형 Q&amp;A 생성 기능</strong
                ><br />생성형 AI가 발표 내용에 대한 예상 질문을 생성하고 질문을
                던져 발표자의 질의응답 대비력을 향상시킵니다.<br />발표가 끝난
                후 사용자가 “질문 있으신가요?” 라는 채팅을 보내면 생성형 AI가
                예상 질문을 던지고 이에 사용자가 답변을 하고 꼬리 질문까지도
                수행할 수 있습니다.<br />이를 통해 사용자는 발표 후 청중과의
                심층적인 질의응답까지 준비할 수 있으며, 질문에 대한 자신감을
                기를 수 있습니다.<br /><br /><br /><strong
                  >6. 연습 기록 저장 및 리포트 기능</strong
                ><br />발표 연습의 모든 피드백 정보를 저장하고, 사용자가 이전
                기록과 비교하며 성장할 수 있도록 리포트 형태로 제공합니다.<br />
              </td>
            </tr>
          </tbody>
        </table>
        <h1 id="1c618ebd-184f-81ff-a106-fbd8b9216c01" class="">
          Project-Design &amp; Implementation
        </h1>
        <table id="1c618ebd-184f-817d-a56d-e5c4947c016d" class="simple-table">
          <tbody>
            <tr id="1c618ebd-184f-81f1-9869-e6850dc2180e">
              <td id="^dlu" class="">항목</td>
              <td id="pTs`" class="" style="width: 1156px">내용</td>
            </tr>
            <tr id="1c618ebd-184f-810f-b0e7-d460c04295be">
              <td id="^dlu" class="">(1) 요구사항 정의</td>
              <td id="pTs`" class="" style="width: 1156px">
                <strong>- 기능별 상세 요구사항</strong><br /><br /><strong
                  >1. 시선 추적 기능</strong
                ><br />OpenCV의 Head Pose Estimation을 이용해 사용자가 발표 중
                아이컨택을 잘 하고 있는지 파악하고, 이를 바탕으로 시선처리
                점수와 피드백 메시지, 아이컨택을 잘 하고 있는지를 실시간으로
                표시한 피드백 영상을 반환합니다. 시선처리 점수, 피드백 메시지,
                아이컨택 여부를 시각화한 영상을 출력합니다.<br /><br /><br /><strong
                  >2. 제스처 인식 기능</strong
                ><br />Google Mediapipe Pose를 이용해 긍정적인 제스처
                2개(설명하는 손동작, 바른 자세), 부정적인 제스처 3개(팔짱 끼는
                동작, 손을 얼굴에 갖다대는 행위, 손을 위협적으로 드는 행위)를
                탐지하고 각 제스처의 빈도수를 측정합니다.<br />각 제스처의
                빈도수를 측정하고, 제스처 점수와 함께 라벨링된 피드백 영상을
                제공합니다.<br /><br /><br /><strong
                  >3. 불필요한 추임새 탐지 기능</strong
                ><br />Google STT와 SpeakUp(오픈소스)을 활용하여 &quot;어&quot;,
                &quot;음&quot;, &quot;그&quot;와 같은 발화 중 발표 유창성을 저해
                불필요한 추임새를 탐지합니다. 이를 바탕으로 발표 유창성 점수와
                피드백 메시지, 어/음/그 각각의 개수 및 발화시간 대비 추임새
                비율을 제공합니다.<br /><br /><br /><strong
                  >4. 대본 유사도 측정 기능</strong
                ><br />사용자가 사전에 입력한 대본과 실제 발표 내용의 유사도를
                cosine 유사도로 계산합니다. 이를 통해 사용자는 발표 흐름을 다시
                점검하고 정확하고 매끄러운 발표를 연습할 수 있습니다.<br /><br /><br /><strong
                  >5. 맞춤형 질의응답 대비 기능</strong
                ><br />발표 종료 후 생성형 AI가 발표 내용을 기반으로 예상 질문을
                생성합니다. 사용자가 응답하면 꼬리 질문을 추가해 심화 Q&amp;A를
                연습할 수 있도록 지원한다.<br /><br /><br /><strong
                  >- 설계 모델</strong
                ><img width="1587" alt="image" src="https://github.com/user-attachments/assets/e3307580-d87f-494a-aa4d-d7a0ffafa56c" /><br/><img width="1527" alt="image" src="https://github.com/user-attachments/assets/1504eabd-6b69-4fed-b06d-e12b64091b6b" />

<br /><br /><strong>- UI 분석/설계 모델</strong
                ><br /><img width="1346" alt="image" src="https://github.com/user-attachments/assets/0625e181-7cdf-458f-b9cb-7ff136d00311" />
<br /><img width="1266" alt="image" src="https://github.com/user-attachments/assets/0a9e2d58-c3f5-4889-8831-ebed5a7e0f2a" />
<br /><br /><strong
                  >- ER 다이어그램/DB 설계 모델(테이블 구조)</strong
                ><br /><img width="1208" alt="image" src="https://github.com/user-attachments/assets/8c7546c3-deb8-41de-9d19-51f5dac75bd2" />
<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-8176-bb92-fc3a4b97683a">
              <td id="^dlu" class="">(2) 전체 시스템 구성</td>
              <td id="pTs`" class="" style="width: 1156px">
                본 프로젝트는 발표 연습 후 사용자의 언어적/비언어적 발표 습관을
                분석하고, 피드백 영상을 제공하는 웹 기반 서비스입니다. 전체
                시스템은 사용자, 프론트엔드, 백엔드 서버, AI 분석 서버, 그리고
                데이터베이스로 구성되며, 각 구성 요소는 다음과 같은 역할을
                수행합니다.<br /><br /><strong
                  >1. 사용자 (User)<br /><br /></strong
                >사용자는 웹사이트를 통해 발표 대본을 등록하고, 연습을 진행하며,
                연습 결과 및 AI 피드백을 확인합니다. 사용자 흐름은 대본 업로드 →
                발표 연습 시작 → (프롬프트 입력 통한) 질의응답 → 연습 종료 →
                결과 확인 순으로 진행됩니다.<br /><br /><br /><strong
                  >2. 프론트엔드 (React 기반 웹 애플리케이션)<br /><br /></strong
                >발표 연습을 위한 영상 촬영 및 대본 보기 UI를 제공하며, 연습
                종료 후 영상과 오디오를 AI 분석 서버로 전송합니다. 이후 분석
                결과를 수신하고 사용자에게 시각적으로 전달합니다. (예: 피드백
                영상, 점수, 그래프 등)<br />사용 기술: React.js (Vite),
                TypeScript, Styled component CSS<br /><br /><br /><strong
                  >3. 백엔드 서버 (Spring Boot)<br /><br /></strong
                >사용자 및 발표 세션을 관리하고, AI 서버로부터 전달받은 분석
                데이터를 저장합니다. 발표 유사도 분석 및 피드백 메시지 생성 또한
                수행하며, 사용자 요청 시 분석 결과를 반환합니다. AI 서버와
                통신하며 결과를 중개하는 핵심 허브 역할을 수행합니다.<br />사용
                기술: Java, Spring Boot, MySQL, AWS RDS<br /><br /><br /><strong
                  >4. AI 분석 서버 (FastAPI + Docker)<br /><br /></strong
                >AI 서버는 총 3가지 분석 기능을 제공합니다. 모든 분석은 연습
                종료 후 후처리 방식으로 이루어집니다.<br />① 비언어적 습관 분석
                (제스처 + 시선 추적): Google Mediapipe Pose와 OpenCV 기반으로
                작동하며, 제스처 분석은 XGBoost 모델을 통해 긍정/부정 동작을
                분류하고, 시선 추적은 Head Pose Estimation과 Eye Contact 감지를
                통해 평가합니다. 결과로 점수, 피드백 메시지, 라벨링된 피드백
                영상을 반환합니다.<br />② 언어적 습관 분석 (STT): Google STT
                API와 TensorFlow 모델을 활용해 사용자의 음성에서
                추임새(&quot;어&quot;, &quot;음&quot;, &quot;그&quot;)를
                탐지하고, 발화 시간 대비 비율을 계산하여 점수를 산출합니다. 전사
                텍스트, 추임새 수, 발화 시간, 점수, 피드백 메시지를 포함한
                JSON을 반환합니다.<br />③ 질의응답 생성 (GPT 기반): 발표
                내용(STT 결과 또는 스크립트 기반)을 바탕으로 GPT API를 통해 예상
                질문을 생성하며, 백엔드에서 결과를 수신하고 프론트에 전달하여
                사용자와의 인터랙션에 활용합니다.<br />사용 기술: FastAPI,
                Python, Docker 기반 컨테이너화 및 AWS 배포, 주요 라이브러리로는
                PyAV, OpenCV, MediaPipe, TensorFlow, pydub, ffmpeg 등을
                사용합니다.<br /><br /><br /><strong
                  >5. 데이터베이스 (MySQL + AWS RDS)<br /><br /></strong
                >사용자 정보, 발표 대본, 연습 세션, 분석 점수, 질문 응답
                데이터를 저장합니다. 주요 테이블은 다음과 같습니다:<br />users:
                사용자 계정 및 설정<br />presentations: 대본 및 제목 등
                메타정보<br />presentations_slides : 발표 슬라이드<br />practice_sessions:
                연습 기록 (날짜, 영상 경로 등)<br />tags : 사용자가 각
                슬라이드마다 주의 사항 및 참고사항을 적어놓을 수 있는 태그<br />feedbacks:
                시선, 제스처, 추임새 분석 결과<br />qa_sessions: GPT 기반 예상
                질문과 사용자의 답변 기록<br /><br /><br /><strong
                  >외부 모듈 및 오픈소스<br /><br /></strong
                >- OpenCV / Mediapipe / XGBoost / TensorFlow: 시선 및 제스처
                분석<br />- Google Speech-to-Text API: 음성 인식 및 추임새
                탐지<br />- ChatGPT API (GPT-4): 챗봇을 통해 청중과의 질의응답을
                연습할 수 있다.<br />- PyAV / ffmpeg: 영상 처리 및 재생 속도
                조절<br />- FastAPI + Docker + AWS: AI 분석 서버 구축 및 배포<br />-
                MySQL + AWS RDS: 데이터 저장 및 관리<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-8126-8b9e-fdb328c65682">
              <td id="^dlu" class="">(3) 주요엔진 및 기능 설계</td>
              <td id="pTs`" class="" style="width: 1156px">
                발표자의 비언어적/언어적 습관을 정량적으로 분석하고, 피드백
                영상을 제공하는 것을 핵심 기능으로 합니다.<br /><br />이를 위해
                AI 분석 서버는 기능별로 분리된 모듈로 설계되었으며, 각 모듈은
                FastAPI 기반 API로 독립 실행되며, 분석 요청을 받아 비디오 및
                오디오 데이터를 처리하고 JSON 및 결과 영상을 반환합니다.<br /><br /><br /><strong
                  >1. 비언어적 습관 분석 모듈<br /><br /></strong
                ><br /><br /><strong>(1) 제스처 분석 (gesture)</strong
                ><br />사용 기술: OpenCV, MediaPipe Pose<br />입력: 연습
                영상(.mp4)<br /><br />처리 흐름:<br />① OpenCV로 프레임 단위로
                영상을 분리<br />② MediaPipe Pose로 각 프레임에서 33개의 신체
                랜드마크 추출<br />③ 추출한 랜드마크 데이터를 정규화하여
                머신러닝 입력값으로 변환<br />④ XGBoost
                모델(gesture_XGB_model.pkl)을 로드하여 제스처 분류 수행<br />⑤
                긍정(설명 손동작, 바른 자세) / 부정(팔짱, 얼굴 터치, 위협적
                손짓) 제스처별 빈도 측정<br />⑥ 분석 결과를 바탕으로 총 제스처
                점수 및 피드백 메시지 생성<br />⑦ 피드백 라벨이 삽입된 영상 생성
                후 반환<br />출력:<br />gesture_score: 총점 및 제스처별 세부
                점수<br />gesture_feedback: 개선 포인트 메시지<br />feedback_video:
                제스처 구간이 시각화된 피드백 영상<br /><br /><br /><strong
                  >(2) 시선 추적 분석 (eyecontact)</strong
                ><br />사용 기술: OpenCV, MediaPipe FaceMesh<br />입력: 연습
                영상(.mp4)<br /><br />처리 흐름:<br />① 영상 프레임 단위 분석<br />②
                FaceMesh로 얼굴 랜드마크 468개 추출<br />③ Head Pose
                Estimation으로 roll, pitch, yaw 계산<br />④ 시선 방향 계산 (정면
                vs 대본 vs 허공)<br />⑤ 정면 주시 시간 비율로 점수 산출<br />⑥
                필요 시 눈 깜빡임(EAR)도 함께 분석하여 집중도 평가<br />⑦
                아이컨택 여부에 따라 구간 라벨링<br />⑧ 결과 기반 피드백 영상
                생성<br /><br />출력:<br />eyecontact_score: 시선 점수 (정면
                주시 비율 기반)<br />eyecontact_feedback: 피드백 메시지<br />feedback_video:
                시선 처리 라벨이 삽입된 피드백 영<br /><br /><br /><strong
                  >2. 언어적 습관 분석 모듈 (stt)</strong
                ><br />사용 기술: speech_recognition, TensorFlow<br />입력: 연습
                영상 (.mp4 형식)<br /><br />처리 흐름:<br />① ffmpeg로 영상에서
                오디오 추출 및 .mp4 → .webm -&gt;.wav 변환<br />② Google STT
                API로 전체 음성 전사 수행<br />③ 추임새(“어”, “음”, “그”) 검출 →
                사전 훈련된 TensorFlow 모델로 분류<br />④ 전체 발화 시간 대비
                추임새 비율 계산<br />⑤ 점수 산출 로직:<br /> - 3% 이하:
                100점<br /> - 3~5%: 감점 (filler_ratio × 3)<br /> - 5~10%: 감점
                (filler_ratio × 3)<br /> - 10% 이상: 감점 (filler_ratio × 4)<br />⑥
                구간별 추임새 하이라이팅 및 전체 텍스트 반환<br /><br />출력:<br />stt_transcript:
                전사본<br />filler_score: 어음그 점수<br />filler_count: 추임새
                개수 및 비율<br />filler_feedback: 피드백 메시지<br /><br /><br /><strong
                  >3. 질의응답 생성 모듈 (gpt)</strong
                ><br />처리 위치: 백엔드(Spring Boot)<br />사용 기술: OpenAI GPT
                API (gpt-4), 프롬프트 엔지니어링<br />입력: 사용자 발표 스크립트
                and STT 결과<br /><br />처리 흐름:<br />① 백엔드에서 GPT API로
                발표 내용을 전송<br />② “이 발표에 대해 청중이 할 수 있는,
                핵심을 관통하는 질문을 정중하게 해주세요” 라는 프롬프트 사용<br />③
                사용자가 응답하면, 추가 꼬리 질문도 함께 생성<br />④ 결과는
                프론트에 전송하여 시뮬레이션 질의응답 UI에서 제공<br /><br /><br /><br /><strong
                  >4. 분석 데이터 흐름 요약</strong
                ><br />프론트엔드에서 연습 종료 후 영상을 FastAPI AI 서버로
                전송<br />AI 서버는 영상 종류에 따라 세 모듈 (gesture.py,
                eyecontact.py, stt.py)을 각각 호출<br />분석된 점수, 피드백
                메시지, 피드백 영상 경로를 백엔드로 전송<br />백엔드는 결과를
                DB에 저장하고 사용자에게 전달<br />질의응답 기능은 백엔드에서
                GPT API 호출을 통해 별도로 작동<br /><br /><br /><br /><strong
                  >5. AI 서버 배포 및 인프라 구조</strong
                ><br />AI 분석 서버는 AWS EC2 인스턴스를 사용해 Docker
                컨테이너화 하여 배포<br />입출력 파일은 AWS S3에 저장<br />입력
                영상은 static/uploads, 출력 영상은 static/outputs 경로에
                저장됨<br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-8142-8d21-c8582c0f33c0">
              <td id="^dlu" class="">(4) 주요 기능의 구현</td>
              <td id="pTs`" class="" style="width: 1156px">
                본 프로젝트의 주요 기능 중 제스처 인식 기능과 STT 기반 추임새
                탐지 기능은 AI 분석 서버에서 구현되며, 연습 영상 및 오디오
                데이터를 입력으로 받아 분석 결과를 JSON 및 영상 형태로
                반환합니다. 다음은 각 기능의 실제 구현 방식입니다.<br /><br /><br /><strong
                  >1. 제스처 인식 기능</strong
                ><br /><br /><strong>기능 개요<br /><br /></strong>사용자의 발표
                영상에서 신체 포즈를 인식하고, 긍정적/부정적 제스처를 분류하여
                빈도와 점수를 계산하며, 피드백 영상을 생성합니다.<br /><br /><br /><strong
                  >주요 구현 흐름<br /><br /></strong
                >1. 사용자가 업로드한 발표 영상을 FastAPI 서버로 전달<br />2.
                OpenCV를 이용해 영상에서 프레임 단위로 이미지 추출<br />3.
                MediaPipe Pose를 사용해 각 프레임의 신체 랜드마크 33개 추출<br />4.
                추출된 좌표를 정규화하여 XGBoost 모델(gesture_XGB_model.pkl)에
                입력<br />5. 각 프레임에 대해 제스처 레이블 예측: <br />
                - 긍정: 설명 손동작, 바른 자세 <br />
                - 부정: 팔짱, 얼굴 터치, 위협적 손짓<br />6. 예측 결과를
                기반으로 프레임별 제스처 카운트 및 등장 시간 계산<br />7.
                제스처별 점수를 산출하고 피드백 메시지 작성 <br />
                - 기본 점수 80점에서 긍정 제스처는 +, 부정 제스처는 - <br />
                - 최종 점수는 정수로 반올림<br />8. 각 프레임에 제스처 라벨을
                시각화하여 영상에 표시<br />9. 피드백 영상과 JSON 결과를
                백엔드에 전송<br /><br /><strong>산출물 예시<br /><br /></strong
                >- gesture_score: 90 <br />- straight_score: 54<br />-
                explain_score: 0<br />- crossed_score: 11<br />- raised_score:
                0<br />- face_score:33<br />- gesture_feedback: &quot;팔짱을
                자주 끼는 습관이 있습니다. 설명하는 손동작을 더 활용하면
                전달력이 좋아집니다.&quot; <br />- feedback_video:
                /static/outputs/filename_gesture_20250330.mp4<br /><br /><strong
                  >2. STT 기반 추임새 탐지 기능</strong
                ><br /><br /><strong>기능 개요</strong> <br />사용자의 발표
                음성을 분석하여 &quot;어&quot;, &quot;음&quot;, &quot;그&quot;와
                같은 추임새를 탐지하고, 발화 시간 대비 비율에 따라 점수를
                계산하며, 전사 텍스트와 함께 피드백을 제공합니다.<br /><br /><strong
                  >주요 구현 흐름</strong
                ><br />1. 프론트엔드에서 업로드된 .mp4 형식 영상 파일 수신 후
                .webm로 변환<br />2. ffmpeg를 활용해 영상에서 오디오 추출 (.wav
                변환)<br />3. Google Speech Recognition API를 사용해 전체 음성
                전사 수행<br />4. TensorFlow로 훈련된 모델을 사용하여 전사된
                문장에서 추임새(&quot;어&quot;, &quot;음&quot;, &quot;그&quot;)
                구간 식별<br />5. pydub과 librosa를 통해 전체 발화 시간과 추임새
                시간 계산<br />6. 추임새 비율(filler_ratio)에 따라 점수 산출
                (로직 기준):<br /><br />추임새 비율에 따른 점수 기준은 다음과
                같습니다.<br /><br />3% 이하: 자연스러운 수준으로 간주되며,
                100점에서 94점 사이의 높은 점수를 부여합니다.<br />3~5%: 다소
                반복적인 추임새 사용으로 판단되며, 91점에서 85점 사이의 점수를
                받습니다.<br />5~10%: 명확성이 떨어지는 수준으로, 84점에서 70점
                사이의 점수를 부여합니다.<br />10% 이상: 발표 흐름을 방해하는
                수준으로 간주되어, 69점 이하의 낮은 점수를 받습니다.<br /><br />7.
                점수에 따라 구간별 피드백 메시지 자동 생성<br />8. JSON 결과를
                백엔드로 전송하며, 전사본에는 추임새 부분을 하이라이팅 표시<br /><br /><strong
                  >산출물 예시</strong
                ><br />- filler_score: 91점 <br />- filler_count: 18개 (2.7%)
                <br />- filler_feedback: &quot;발화 중 추임새가 자주
                사용되었습니다. 발표 흐름을 더 자연스럽게 이어가는 연습이
                필요합니다.&quot; <br />- transcript_highlighted: &quot;오늘은
                **그...** 기후 변화에 대해 **음...** 이야기하겠습니다.&quot;
                <br />
              </td>
            </tr>
            <tr id="1c618ebd-184f-816b-8d67-d50be1b0ccf9">
              <td id="^dlu" class="">(5) 기타</td>
              <td id="pTs`" class="" style="width: 1156px"></td>
            </tr>
          </tbody>
        </table>
      </div>
    </article>
    <span class="sans" style="font-size: 14px; padding-top: 2em"></span>
  </body>
</html>
